{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0d0f9a6-6b51-42f6-b7f6-7e7a9bdda2bb",
   "metadata": {},
   "source": [
    "## 使用Hugging Face的tokenizers库子词词元化\n",
    "\n",
    "`tokenizers`是`Hugging Face`官方使用`Rust`编写的分词工具，提供了`Python`和`JavaScript`语言的接口调用。\n",
    "\n",
    "安装：`pip install tokenizers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "289c3774-0e49-4a79-beae-774d3dc1cd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d90b4377-6a47-4a73-9f30-936940724aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curated-tokenizers==0.0.9\n",
      "tokenizers==0.15.2\n"
     ]
    }
   ],
   "source": [
    "!pip freeze | grep tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0453abf9-3f2e-44a4-a172-2a682c90e75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6abf4e2-fac5-4b52-90aa-aed9e3849ae6",
   "metadata": {},
   "source": [
    "我们可以搭建自己的分词器：https://huggingface.co/docs/tokenizers/main/en/quicktour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce249427-3a4e-447c-a514-d7b0b0fb49d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4e71d27-4d8d-4fe8-9cda-1150cc8b509f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第一次会下载模型，会有一个进度条\n",
    "# 后面再次运行，一般就是从缓存中加载模型了，缓存一般在：~/.cache/huggingface目录中\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad1ae20-6654-4f4b-9753-1ebb566f9e85",
   "metadata": {},
   "source": [
    "**次元分析器的重要属性：** 词表的大小、模型的最大上下文。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88d32bd7-2094-4046-bbfb-42ef17c30cc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30522"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 词表的大小\n",
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d54086e-48b2-4460-b72e-795d53cd0c10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 模型的最大上下文大小\n",
    "tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce31f17f-41c6-4672-adc7-0eb4dd648de2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['input_ids', 'token_type_ids', 'attention_mask']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 模型在前向传播中期望的字段名称\n",
    "tokenizer.model_input_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6027fd8d-6996-4845-9e00-a66d9e5c8895",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vocab_file': 'vocab.txt', 'tokenizer_file': 'tokenizer.json'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看此表文件名称\n",
    "tokenizer.vocab_files_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977b6bec-4ff5-4976-9221-580ad5201579",
   "metadata": {},
   "source": [
    "> 我们可以去huggingface.co网站上去查看[`bert-base-uncased`模型的文件列表](https://huggingface.co/google-bert/bert-base-uncased/tree/main)。\n",
    "> 里面会有`vocab.txt`和`tokenizer.json`文件。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b99feb7-3090-4881-87db-6264c7d3a747",
   "metadata": {},
   "source": [
    "**下面让我们使用这个分词器，对文本词元化。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "365c15b9-a8e6-42fe-a194-efa8fb54b4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"I love python and transformer.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da224954-24d3-4ace-833a-eb7414d8130b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 1045, 2293, 18750, 1998, 10938, 2121, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_text = tokenizer(text)\n",
    "encoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1173bdac-56a4-442d-9617-54909c473c50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 1045, 2293, 18750, 1998, 10938, 2121, 1012, 102]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d7bcf2-8b73-4d5f-bf60-904256e81d57",
   "metadata": {},
   "source": [
    "> 通过上面直接调用`tokenizer()`和`tokenizer.encode()`其返回值是不一样的。    \n",
    "> 如果只关注次元的数字ID，可以用encode。应用中我们一般会使用`tokenizer()`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a1807c2-c090-4b05-8772-2da6a9b437e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'love', 'python', 'and', 'transform', '##er', '.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f7ceb81f-119e-412d-b5b0-091fcabc3311",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] i love python and transformer. [SEP] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 把token的id列表，解码为文本\n",
    "# 输出的时候：##会和前面的token相连接\n",
    "tokenizer.decode([101, 1045, 2293, 18750, 1998, 10938, 2121, 1012, 102, 0, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "45b4fb5e-3687-4bcd-aa37-a41b18ce82a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[UNK] [CLS] [SEP] [MASK] [PAD]'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 把token的ids转换为字符串token\n",
    "tokenizer.decode([100, 101, 102, 103, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7927ee36-11c4-42ed-a6c7-2196ca5e2fd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[UNK]', '[CLS]', '[SEP]', '[MASK]', '[PAD]']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 除了decode方法也可以通过convert_ids_to_tokens方法\n",
    "# 不过前者返回的是一个字符串文本，后者返回的是token的字符串列表\n",
    "tokenizer.convert_ids_to_tokens([100, 101, 102, 103, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042a7c14-bb4a-4a9e-86f8-c91b59a237e8",
   "metadata": {},
   "source": [
    "**直接返回张量：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "65b4f814-616a-40e6-913f-7cb9dd3a3f86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  1045,  2293, 18750,  1998, 10938,  2121,  1012,   102,     0,\n",
       "             0,     0,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(text, padding=\"max_length\", max_length=15, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8a0df2cc-b81e-49a1-a3a6-cd9ed05e505c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101,  1045,  2293, 18750,  1998, 10938,  2121,  1012,   102,     0,\n",
       "             0,     0,     0,     0,     0]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(text, padding=\"max_length\", max_length=15, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed607e9-b35b-44d4-bb7c-4cdea423b8b2",
   "metadata": {},
   "source": [
    "**直接返回张量，且填充为15的长度**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d999df5e-d31a-4a74-84ca-2c9e85817d16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vocab_file': 'vocab.txt', 'tokenizer_file': 'tokenizer.json'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_files_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1d94be-bd8c-46b5-ab4f-fdbd68f7ed6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
